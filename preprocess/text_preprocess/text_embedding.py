import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import psycopg2
from sqlalchemy import create_engine
from sentence_transformers import SentenceTransformer
import h5py
from dotenv import load_dotenv
import warnings

warnings.filterwarnings("ignore")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# utils ëª¨ë“ˆì—ì„œ classify_columns í•¨ìˆ˜ import
from preprocess.utils.classify_columns import classify_columns


class TextEmbeddingPipeline:
    """í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ìë™ Dense Embedding íŒŒì´í”„ë¼ì¸"""

    def __init__(self, db_config: Dict, model_name: str = "klue/roberta-base"):
        """
        Args:
            db_config: PostgreSQL ì—°ê²° ì •ë³´
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.db_config = db_config
        self.model_name = model_name
        self.engine = None
        self.model = None
        self.text_columns = []
        self.embeddings_dict = {}

        self._connect_db()
        self._load_model()
        self._identify_text_columns()

    def _connect_db(self):
        """PostgreSQL ì—°ê²°"""
        try:
            conn_str = (
                f"postgresql://{self.db_config['user']}:"
                f"{self.db_config['password']}@"
                f"{self.db_config['host']}:"
                f"{self.db_config['port']}/"
                f"{self.db_config['database']}"
            )
            self.engine = create_engine(conn_str)
            print("âœ… PostgreSQL ì—°ê²° ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

    def _load_model(self):
        """ì„ë² ë”© ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ”„ {self.model_name} ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = SentenceTransformer(self.model_name)
            print(
                f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ! ì„ë² ë”© ì°¨ì›: {self.model.get_sentence_embedding_dimension()}"
            )
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _identify_text_columns(self):
        """ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•´ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì‹ë³„"""
        try:
            # notice.csv ë©”íƒ€ë°ì´í„° ë¡œë“œ
            meta_path = project_root / "preprocess" / "meta" / "notice.csv"
            meta_df = pd.read_csv(meta_path, dtype=str)

            # ì»¬ëŸ¼ ë¶„ë¥˜
            column_groups = classify_columns(meta_df)
            self.text_columns = column_groups.get("text", [])

            print(f"ğŸ“Š ì‹ë³„ëœ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ({len(self.text_columns)}ê°œ):")
            for col in self.text_columns:
                print(f"  - {col}")

        except Exception as e:
            print(f"âŒ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì‹ë³„ ì‹¤íŒ¨: {e}")
            raise

    def load_data(self, limit: Optional[int] = None, sample_random: bool = True):
        """
        notice í…Œì´ë¸”ì—ì„œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ë°ì´í„° ë¡œë“œ

        Args:
            limit: ë¡œë“œí•  ë°ì´í„° ê°œìˆ˜ ì œí•œ
            sample_random: ëœë¤ ìƒ˜í”Œë§ ì—¬ë¶€
        """
        try:
            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ë“¤ë§Œ SELECT
            text_cols_str = ", ".join(self.text_columns)

            base_query = f"""
            SELECT {text_cols_str}
            FROM notice 
            WHERE 1=1
            """

            # ìµœì†Œí•œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ NULLì´ ì•„ë‹Œ ì¡°ê±´ ì¶”ê°€
            conditions = []
            for col in self.text_columns:
                conditions.append(f"({col} IS NOT NULL AND TRIM({col}) != '')")

            if conditions:
                base_query += f" AND ({' OR '.join(conditions)})"

            # ëœë¤ ìƒ˜í”Œë§ ë˜ëŠ” ìµœì‹ ìˆœ
            if sample_random:
                base_query += " ORDER BY RANDOM()"
            else:
                base_query += " ORDER BY id DESC"

            # LIMIT ì¶”ê°€
            if limit:
                base_query += f" LIMIT {limit}"

            print(f"ğŸ”„ í…ìŠ¤íŠ¸ ë°ì´í„° ë¡œë”© ì¤‘... (limit: {limit})")
            print(f"ğŸ“„ ì¿¼ë¦¬: {base_query[:100]}...")

            self.df = pd.read_sql(base_query, self.engine)

            print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
            print(f"   - ë¡œë“œëœ ë°ì´í„°: {len(self.df)}ê°œ")
            print(f"   - í…ìŠ¤íŠ¸ ì»¬ëŸ¼: {len(self.text_columns)}ê°œ")

            # ê° ì»¬ëŸ¼ë³„ NULL ë¹„ìœ¨ ì²´í¬
            self._analyze_null_pattern()

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _analyze_null_pattern(self):
        """ê° í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì˜ NULL íŒ¨í„´ ë¶„ì„"""
        print("\nğŸ“Š í…ìŠ¤íŠ¸ ì»¬ëŸ¼ NULL ë¶„ì„:")
        print("-" * 60)

        for col in self.text_columns:
            if col in self.df.columns:
                total = len(self.df)
                null_count = self.df[col].isnull().sum()
                empty_count = (self.df[col] == "").sum()
                valid_count = total - null_count - empty_count

                print(
                    f"{col:20s}: "
                    f"ìœ íš¨ {valid_count:6d}ê°œ ({valid_count/total*100:5.1f}%) | "
                    f"NULL {null_count:6d}ê°œ | "
                    f"ë¹ˆê°’ {empty_count:6d}ê°œ"
                )

    def _preprocess_text_column(
        self, column_data: pd.Series, column_name: str
    ) -> Tuple[List[str], List[int]]:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì „ì²˜ë¦¬"""
        processed_texts = []
        valid_indices = []

        for i, text in enumerate(column_data):
            if pd.notna(text) and str(text).strip() != "":
                # ê¸°ë³¸ ì „ì²˜ë¦¬
                cleaned_text = str(text).strip()
                # ì¶”ê°€ ì „ì²˜ë¦¬ (í•„ìš”ì‹œ)
                cleaned_text = self._clean_text(cleaned_text)

                if len(cleaned_text) >= 2:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    processed_texts.append(cleaned_text)
                    valid_indices.append(i)

        print(f"   {column_name}: {len(column_data)} â†’ {len(processed_texts)}ê°œ ìœ íš¨")
        return processed_texts, valid_indices

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        import re

        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ ê¸°í˜¸ë§Œ ìœ ì§€)
        text = re.sub(r"[^\w\sê°€-í£().-]", " ", text)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def create_embeddings(self, batch_size: int):
        """ëª¨ë“  í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì— ëŒ€í•´ ì„ë² ë”© ìƒì„±"""
        try:
            print(f"\nğŸ”„ ì„ë² ë”© ìƒì„± ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})")
            print("=" * 80)

            for col in self.text_columns:
                if col not in self.df.columns:
                    print(f"âš ï¸  {col} ì»¬ëŸ¼ì´ ë°ì´í„°í”„ë ˆì„ì— ì—†ìŒ")
                    continue

                print(f"\nğŸ“ {col} ì»¬ëŸ¼ ì„ë² ë”© ìƒì„± ì¤‘...")

                # ì „ì²˜ë¦¬
                processed_texts, valid_indices = self._preprocess_text_column(
                    self.df[col], col
                )

                if len(processed_texts) == 0:
                    print(f"   âš ï¸ {col}: ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŒ")
                    continue

                # ì„ë² ë”© ìƒì„±
                embeddings = self.model.encode(
                    processed_texts,
                    batch_size=batch_size,
                    show_progress_bar=True,
                    convert_to_numpy=True,
                )

                # ì›ë³¸ í¬ê¸°ë¡œ ë³µì› (NULL ìœ„ì¹˜ëŠ” 0ë²¡í„°)
                full_embeddings = np.zeros((len(self.df), embeddings.shape[1]))
                full_embeddings[valid_indices] = embeddings

                # ì €ì¥
                self.embeddings_dict[col] = {
                    "embeddings": full_embeddings,
                    "valid_indices": valid_indices,
                    "valid_count": len(valid_indices),
                    "embedding_dim": embeddings.shape[1],
                }

                print(
                    f"   âœ… {col}: {embeddings.shape[0]}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ "
                    f"(ì°¨ì›: {embeddings.shape[1]})"
                )

            print(f"\nğŸ‰ ì „ì²´ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
            self._print_embedding_summary()

        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def _print_embedding_summary(self):
        """ì„ë² ë”© ìƒì„± ê²°ê³¼ ìš”ì•½"""
        print("\nğŸ“Š ì„ë² ë”© ìƒì„± ìš”ì•½:")
        print("-" * 80)

        total_embeddings = 0
        for col, info in self.embeddings_dict.items():
            print(
                f"{col:20s}: "
                f"{info['valid_count']:6d}ê°œ ì„ë² ë”© | "
                f"ì°¨ì›: {info['embedding_dim']:4d} | "
                f"í¬ê¸°: {info['embeddings'].nbytes / (1024*1024):6.1f}MB"
            )
            total_embeddings += info["valid_count"]

        print(f"\nì´ ìƒì„±ëœ ì„ë² ë”©: {total_embeddings:,}ê°œ")

    def save_embeddings(self, output_dir: str = "embeddings_output"):
        """ì„ë² ë”© ê²°ê³¼ ì €ì¥"""
        try:
            output_path = Path(output_dir)
            output_path.mkdir(exist_ok=True)

            print(f"\nğŸ’¾ ì„ë² ë”© ì €ì¥ ì¤‘... ({output_path})")

            # HDF5 í˜•íƒœë¡œ ì••ì¶• ì €ì¥
            h5_path = (
                output_path
                / f"notice_embeddings_{self.model_name.replace('/', '_')}.h5"
            )

            with h5py.File(h5_path, "w") as f:
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                f.attrs["model_name"] = self.model_name
                f.attrs["data_count"] = len(self.df)
                f.attrs["text_columns"] = [
                    col.encode("utf-8") for col in self.text_columns
                ]

                # ê° ì»¬ëŸ¼ë³„ ì„ë² ë”© ì €ì¥
                for col, info in self.embeddings_dict.items():
                    group = f.create_group(col)
                    group.create_dataset(
                        "embeddings", data=info["embeddings"], compression="gzip"
                    )
                    group.create_dataset(
                        "valid_indices", data=info["valid_indices"], compression="gzip"
                    )
                    group.attrs["valid_count"] = info["valid_count"]
                    group.attrs["embedding_dim"] = info["embedding_dim"]

            # CSVë¡œ ë©”íƒ€ë°ì´í„° ì €ì¥
            meta_info = []
            for col, info in self.embeddings_dict.items():
                meta_info.append(
                    {
                        "column_name": col,
                        "valid_count": info["valid_count"],
                        "embedding_dim": info["embedding_dim"],
                        "file_size_mb": info["embeddings"].nbytes / (1024 * 1024),
                    }
                )

            meta_df = pd.DataFrame(meta_info)
            meta_df.to_csv(output_path / "embedding_metadata.csv", index=False)

            print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ!")
            print(f"   - HDF5 íŒŒì¼: {h5_path}")
            print(f"   - ë©”íƒ€ë°ì´í„°: {output_path / 'embedding_metadata.csv'}")
            print(f"   - ì´ íŒŒì¼ í¬ê¸°: {h5_path.stat().st_size / (1024*1024):.1f}MB")

        except Exception as e:
            print(f"âŒ ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {e}")
            raise

    def load_saved_embeddings(self, h5_path: str) -> Dict:
        """ì €ì¥ëœ ì„ë² ë”© ë¡œë“œ"""
        try:
            print(f"ğŸ“‚ ì„ë² ë”© ë¡œë“œ ì¤‘... ({h5_path})")

            embeddings_dict = {}

            with h5py.File(h5_path, "r") as f:
                model_name = f.attrs["model_name"]
                data_count = f.attrs["data_count"]

                print(f"   ëª¨ë¸: {model_name}")
                print(f"   ë°ì´í„° ê°œìˆ˜: {data_count:,}ê°œ")

                for col_name in f.keys():
                    group = f[col_name]
                    embeddings_dict[col_name] = {
                        "embeddings": group["embeddings"][:],
                        "valid_indices": group["valid_indices"][:],
                        "valid_count": group.attrs["valid_count"],
                        "embedding_dim": group.attrs["embedding_dim"],
                    }

                    print(f"   {col_name}: {group.attrs['valid_count']}ê°œ ì„ë² ë”©")

            print("âœ… ì„ë² ë”© ë¡œë“œ ì™„ë£Œ!")
            return embeddings_dict

        except Exception as e:
            print(f"âŒ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    # PostgreSQL ì„¤ì •
    db_config = {
        "host": os.getenv("POSTGRES_HOST", "localhost"),
        "port": int(os.getenv("POSTGRES_PORT", 5432)),
        "user": os.getenv("POSTGRES_USER"),
        "password": os.getenv("POSTGRES_PASSWORD"),
        "database": os.getenv("POSTGRES_DB"),
    }

    try:
        print("ğŸš€ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ Dense Embedding íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 80)

        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (KoELECTRA ì‚¬ìš©)
        pipeline = TextEmbeddingPipeline(
            db_config=db_config, model_name="monologg/koelectra-base-v3-discriminator"
        )

        # ë°ì´í„° ë¡œë“œ (í…ŒìŠ¤íŠ¸ìš© 1000ê°œ)
        pipeline.load_data(limit=1000, sample_random=True)

        # ì„ë² ë”© ìƒì„±
        pipeline.create_embeddings(batch_size=10)

        # ê²°ê³¼ ì €ì¥
        pipeline.save_embeddings("notice_text_embeddings")

        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")

        # ì‚¬ìš© ì˜ˆì‹œ
        print("\nğŸ’¡ ì„ë² ë”© ì‚¬ìš© ì˜ˆì‹œ:")
        print("# íŠ¹ì • ì»¬ëŸ¼ ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°")
        print(
            "bidntcenm_embeddings = pipeline.embeddings_dict['bidntcenm']['embeddings']"
        )
        print("print(f'Shape: {bidntcenm_embeddings.shape}')")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
