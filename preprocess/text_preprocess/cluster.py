import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
import psycopg2
from sqlalchemy import create_engine
import re
import os
from dotenv import load_dotenv
from typing import List, Dict, Optional
import warnings

warnings.filterwarnings("ignore")


class NoticeClusteringPipeline:
    """ê³µê³ ì œëª© í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸"""

    def __init__(self, db_config: Dict):
        """
        Args:
            db_config: PostgreSQL ì—°ê²° ì •ë³´
        """
        self.db_config = db_config
        self.engine = None
        self.model = None
        self.df = None
        self.embeddings = None
        self.cluster_labels = None
        self.cluster_results = None

        self._connect_db()
        self._load_model()

    def _connect_db(self):
        """PostgreSQL ì—°ê²°"""
        try:
            conn_str = (
                f"postgresql://{self.db_config['user']}:"
                f"{self.db_config['password']}@"
                f"{self.db_config['host']}:"
                f"{self.db_config['port']}/"
                f"{self.db_config['database']}"
            )
            self.engine = create_engine(conn_str)
            print("âœ… PostgreSQL ì—°ê²° ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            raise

    def _load_model(self):
        """KoELECTRA ëª¨ë¸ ë¡œë“œ"""
        try:
            print("ğŸ”„ KoELECTRA ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = SentenceTransformer("monologg/koelectra-base-v3-discriminator")
            print("âœ… KoELECTRA ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def load_data(self, limit: int = 10000, sample_random: bool = True):
        """
        ê³µê³ ì œëª© ë°ì´í„° ë¡œë“œ

        Args:
            limit: ê°€ì ¸ì˜¬ ë°ì´í„° ê°œìˆ˜
            sample_random: ëœë¤ ìƒ˜í”Œë§ ì—¬ë¶€
        """
        try:
            if sample_random:
                query = f"""
                SELECT bidntcenm 
                FROM notice 
                WHERE bidntcenm IS NOT NULL 
                AND TRIM(bidntcenm) != ''
                AND LENGTH(bidntcenm) >= 10
                ORDER BY RANDOM()
                LIMIT {limit}
                """
            else:
                query = f"""
                SELECT bidntcenm 
                FROM notice 
                WHERE bidntcenm IS NOT NULL 
                AND TRIM(bidntcenm) != ''
                AND LENGTH(bidntcenm) >= 10
                ORDER BY id DESC
                LIMIT {limit}
                """

            print(f"ğŸ”„ ê³µê³ ì œëª© ë°ì´í„° ë¡œë”© ì¤‘... (limit: {limit})")
            self.df = pd.read_sql(query, self.engine)

            # ì¤‘ë³µ ì œê±° ë° ì „ì²˜ë¦¬
            initial_count = len(self.df)
            self.df = self.df.drop_duplicates(subset=["bidntcenm"])
            self.df["bidntcenm"] = self.df["bidntcenm"].apply(self._preprocess_title)
            self.df = self.df[
                self.df["bidntcenm"].str.len() >= 5
            ]  # ë„ˆë¬´ ì§§ì€ ì œëª© ì œê±°

            final_count = len(self.df)
            print(f"âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
            print(f"   - ì´ˆê¸°: {initial_count}ê°œ")
            print(f"   - ì¤‘ë³µì œê±° ë° ì „ì²˜ë¦¬ í›„: {final_count}ê°œ")
            print(f"   - ìƒ˜í”Œ ì œëª©: {self.df['bidntcenm'].iloc[0]}")

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    def _preprocess_title(self, title: str) -> str:
        """ì œëª© ì „ì²˜ë¦¬"""
        if pd.isna(title):
            return ""

        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬ (í•œê¸€, ì˜ì–´, ìˆ«ì, ê¸°ë³¸ ê¸°í˜¸ë§Œ ìœ ì§€)
        title = re.sub(r"[^\w\sê°€-í£().-]", " ", title)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        title = re.sub(r"\s+", " ", title)
        return title.strip()

    def create_embeddings(self, batch_size: int = 32):
        """ì„ë² ë”© ìƒì„±"""
        try:
            print("ğŸ”„ ì„ë² ë”© ìƒì„± ì¤‘...")
            titles = self.df["bidntcenm"].tolist()

            self.embeddings = self.model.encode(
                titles,
                batch_size=batch_size,
                show_progress_bar=True,
                convert_to_numpy=True,
            )

            print(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ! Shape: {self.embeddings.shape}")

        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    def perform_clustering(self, n_clusters: int = 50, random_state: int = 42):
        """K-means í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰"""
        try:
            print(f"ğŸ”„ í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ ì¤‘... (n_clusters: {n_clusters})")

            # K-means í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(
                n_clusters=n_clusters,
                random_state=random_state,
                n_init=10,
                max_iter=300,
            )

            self.cluster_labels = kmeans.fit_predict(self.embeddings)

            # í´ëŸ¬ìŠ¤í„° ì •ë³´ë¥¼ DataFrameì— ì¶”ê°€
            self.df["cluster"] = self.cluster_labels

            print(f"âœ… í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")
            print(f"   - ì´ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(np.unique(self.cluster_labels))}")
            print(
                f"   - ê°€ì¥ í° í´ëŸ¬ìŠ¤í„° í¬ê¸°: {np.max(np.bincount(self.cluster_labels))}"
            )

        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
            raise

    def analyze_clusters(self, top_k_keywords: int = 5, top_k_clusters: int = 20):
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ ë° í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            print("ğŸ”„ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì¤‘...")

            results = []
            total_size = len(self.df)

            for cluster_id in range(len(np.unique(self.cluster_labels))):
                cluster_data = self.df[self.df["cluster"] == cluster_id]
                cluster_titles = cluster_data["bidntcenm"].tolist()
                cluster_size = len(cluster_titles)

                if cluster_size == 0:
                    continue

                # TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
                tfidf_keywords = self._extract_tfidf_keywords(
                    cluster_titles, top_k_keywords
                )

                # ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ
                freq_keywords = self._extract_frequent_keywords(
                    cluster_titles, top_k_keywords
                )

                # ìƒ˜í”Œ ì œëª©ë“¤
                samples = (
                    cluster_titles[:3] if len(cluster_titles) >= 3 else cluster_titles
                )

                results.append(
                    {
                        "cluster_id": cluster_id,
                        "size": cluster_size,
                        "percentage": (cluster_size / total_size) * 100,
                        "tfidf_keywords": tfidf_keywords,
                        "frequent_keywords": freq_keywords,
                        "samples": samples,
                        "avg_length": np.mean([len(title) for title in cluster_titles]),
                    }
                )

            # í¬ê¸°ìˆœìœ¼ë¡œ ì •ë ¬
            results.sort(key=lambda x: x["size"], reverse=True)
            self.cluster_results = results

            # ê²°ê³¼ ì¶œë ¥
            self._print_cluster_analysis(results[:top_k_clusters])

        except Exception as e:
            print(f"âŒ í´ëŸ¬ìŠ¤í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            raise

    def _extract_tfidf_keywords(self, titles: List[str], top_k: int = 5):
        """TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            if len(titles) < 2:
                return []

            # ê°„ë‹¨í•œ ì „ì²˜ë¦¬
            processed_titles = [self._simple_preprocess(title) for title in titles]

            vectorizer = TfidfVectorizer(
                max_features=1000, ngram_range=(1, 2), min_df=2, max_df=0.95
            )

            tfidf_matrix = vectorizer.fit_transform(processed_titles)
            feature_names = vectorizer.get_feature_names_out()

            # í‰ê·  TF-IDF ì ìˆ˜ ê³„ì‚°
            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)

            # ìƒìœ„ í‚¤ì›Œë“œ
            top_indices = np.argsort(mean_scores)[-top_k:][::-1]
            keywords = [(feature_names[i], mean_scores[i]) for i in top_indices]

            return keywords

        except Exception:
            return []

    def _extract_frequent_keywords(self, titles: List[str], top_k: int = 5):
        """ë¹ˆë„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            words = []
            for title in titles:
                # ê°„ë‹¨í•œ ë‹¨ì–´ ë¶„ë¦¬ (ê³µë°± ê¸°ì¤€)
                cleaned = re.sub(r"[^\w\sê°€-í£]", " ", title)
                words.extend([word for word in cleaned.split() if len(word) >= 2])

            # ë¹ˆë„ ê³„ì‚°
            word_counts = Counter(words)
            return word_counts.most_common(top_k)

        except Exception:
            return []

    def _simple_preprocess(self, text: str) -> str:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        text = re.sub(r"[^\w\sê°€-í£]", " ", text)
        text = re.sub(r"\s+", " ", text)
        return text.strip()

    def _print_cluster_analysis(self, results: List[Dict]):
        """í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "=" * 100)
        print("ğŸ“Š í´ëŸ¬ìŠ¤í„° ë¶„ì„ ê²°ê³¼")
        print("=" * 100)

        for i, cluster in enumerate(results):
            print(
                f"\nğŸ”¸ í´ëŸ¬ìŠ¤í„° {cluster['cluster_id']} (í¬ê¸°: {cluster['size']}ê°œ, {cluster['percentage']:.1f}%, í‰ê· ê¸¸ì´: {cluster['avg_length']:.1f}ì)"
            )

            # TF-IDF í‚¤ì›Œë“œ
            if cluster["tfidf_keywords"]:
                tfidf_str = ", ".join(
                    [
                        f"{word}({score:.3f})"
                        for word, score in cluster["tfidf_keywords"]
                    ]
                )
                print(f"   ğŸ”‘ TF-IDF: {tfidf_str}")

            # ë¹ˆë„ í‚¤ì›Œë“œ
            if cluster["frequent_keywords"]:
                freq_str = ", ".join(
                    [f"{word}({count})" for word, count in cluster["frequent_keywords"]]
                )
                print(f"   ğŸ“ˆ ë¹ˆë„: {freq_str}")

            # ìƒ˜í”Œ ì œëª©
            print(f"   ğŸ“ ìƒ˜í”Œ:")
            for j, sample in enumerate(cluster["samples"], 1):
                print(f"      {j}. {sample}")

            if i >= 19:  # ìƒìœ„ 20ê°œë§Œ ì¶œë ¥
                break

    def visualize_clusters(self, figsize: tuple = (15, 10)):
        """í´ëŸ¬ìŠ¤í„° ì‹œê°í™”"""
        try:
            print("ğŸ”„ í´ëŸ¬ìŠ¤í„° ì‹œê°í™” ì¤‘...")

            # PCAë¡œ ì°¨ì› ì¶•ì†Œ
            pca = PCA(n_components=2, random_state=42)
            embeddings_2d = pca.fit_transform(self.embeddings)

            # ì‹œê°í™”
            fig, axes = plt.subplots(2, 2, figsize=figsize)

            # 1. í´ëŸ¬ìŠ¤í„° ì‚°ì ë„
            scatter = axes[0, 0].scatter(
                embeddings_2d[:, 0],
                embeddings_2d[:, 1],
                c=self.cluster_labels,
                cmap="tab20",
                alpha=0.6,
                s=1,
            )
            axes[0, 0].set_title("í´ëŸ¬ìŠ¤í„° ë¶„í¬ (PCA)")
            axes[0, 0].set_xlabel("PC1")
            axes[0, 0].set_ylabel("PC2")

            # 2. í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
            cluster_sizes = [result["size"] for result in self.cluster_results[:20]]
            cluster_ids = [
                f"C{result['cluster_id']}" for result in self.cluster_results[:20]
            ]

            axes[0, 1].bar(range(len(cluster_sizes)), cluster_sizes)
            axes[0, 1].set_title("ìƒìœ„ 20ê°œ í´ëŸ¬ìŠ¤í„° í¬ê¸°")
            axes[0, 1].set_xlabel("í´ëŸ¬ìŠ¤í„°")
            axes[0, 1].set_ylabel("ë¬¸ì„œ ìˆ˜")
            axes[0, 1].tick_params(axis="x", rotation=45)

            # 3. í´ëŸ¬ìŠ¤í„° í¬ê¸° íˆìŠ¤í† ê·¸ë¨
            all_sizes = [result["size"] for result in self.cluster_results]
            axes[1, 0].hist(all_sizes, bins=30, alpha=0.7)
            axes[1, 0].set_title("í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬")
            axes[1, 0].set_xlabel("í´ëŸ¬ìŠ¤í„° í¬ê¸°")
            axes[1, 0].set_ylabel("ë¹ˆë„")

            # 4. ëˆ„ì  ì»¤ë²„ë¦¬ì§€
            cumulative_percentage = np.cumsum(
                [result["percentage"] for result in self.cluster_results]
            )
            axes[1, 1].plot(
                range(1, len(cumulative_percentage) + 1), cumulative_percentage
            )
            axes[1, 1].set_title("ëˆ„ì  ì»¤ë²„ë¦¬ì§€")
            axes[1, 1].set_xlabel("í´ëŸ¬ìŠ¤í„° ìˆœìœ„")
            axes[1, 1].set_ylabel("ëˆ„ì  ë¹„ìœ¨ (%)")
            axes[1, 1].grid(True, alpha=0.3)

            plt.tight_layout()
            plt.show()

            print("âœ… ì‹œê°í™” ì™„ë£Œ!")

        except Exception as e:
            print(f"âŒ ì‹œê°í™” ì‹¤íŒ¨: {e}")

    def save_results(self, output_path: str = "cluster_results.csv"):
        """ê²°ê³¼ ì €ì¥"""
        try:
            # í´ëŸ¬ìŠ¤í„° ì •ë³´ê°€ í¬í•¨ëœ DataFrame ì €ì¥
            self.df.to_csv(output_path, index=False, encoding="utf-8")
            print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")

            # í´ëŸ¬ìŠ¤í„° ìš”ì•½ ì •ë³´ ì €ì¥
            summary_path = output_path.replace(".csv", "_summary.txt")
            with open(summary_path, "w", encoding="utf-8") as f:
                f.write("í´ëŸ¬ìŠ¤í„° ë¶„ì„ ìš”ì•½\n")
                f.write("=" * 50 + "\n\n")

                for result in self.cluster_results[:20]:
                    f.write(
                        f"í´ëŸ¬ìŠ¤í„° {result['cluster_id']} (í¬ê¸°: {result['size']}ê°œ)\n"
                    )
                    f.write(f"TF-IDF: {result['tfidf_keywords']}\n")
                    f.write(
                        f"ìƒ˜í”Œ: {result['samples'][0] if result['samples'] else 'N/A'}\n"
                    )
                    f.write("-" * 30 + "\n")

            print(f"âœ… ìš”ì•½ ì €ì¥ ì™„ë£Œ: {summary_path}")

        except Exception as e:
            print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    # í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()

    # PostgreSQL ì„¤ì •
    db_config = {
        "host": os.getenv("POSTGRES_HOST", "localhost"),
        "port": int(os.getenv("POSTGRES_PORT", 5432)),
        "user": os.getenv("POSTGRES_USER"),
        "password": os.getenv("POSTGRES_PASSWORD"),
        "database": os.getenv("POSTGRES_DB"),
    }

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    try:
        print("ğŸš€ ê³µê³ ì œëª© í´ëŸ¬ìŠ¤í„°ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("=" * 50)

        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = NoticeClusteringPipeline(db_config)

        # ë°ì´í„° ë¡œë“œ (10,000ê°œë¡œ ì‹œì‘)
        pipeline.load_data(limit=10000, sample_random=True)

        # ì„ë² ë”© ìƒì„±
        pipeline.create_embeddings(batch_size=32)

        # í´ëŸ¬ìŠ¤í„°ë§ (50ê°œ í´ëŸ¬ìŠ¤í„°)
        pipeline.perform_clustering(n_clusters=50)

        # í´ëŸ¬ìŠ¤í„° ë¶„ì„
        pipeline.analyze_clusters(top_k_keywords=5, top_k_clusters=20)

        # ì‹œê°í™”
        pipeline.visualize_clusters()

        # ê²°ê³¼ ì €ì¥
        pipeline.save_results("notice_clustering_results.csv")

        print("\nğŸ‰ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise


if __name__ == "__main__":
    main()
